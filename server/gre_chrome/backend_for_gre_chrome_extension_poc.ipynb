{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "injured-throat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "earlier-glass",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3626"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    all_words = pickle.load(open('really_all_words.pkl','rb'))\n",
    "except FileNotFoundError:\n",
    "    all_words = []\n",
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alpha-version",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['List01.txt',\n",
       " 'word_list_magoosh.txt',\n",
       " 'word_list_pylike.txt',\n",
       " 'List05.txt',\n",
       " 'List09.txt',\n",
       " 'List08.txt',\n",
       " 'List04.txt',\n",
       " 'List07.txt',\n",
       " 'List02.txt',\n",
       " 'word_list_essential.txt',\n",
       " 'words_with_meanings.json',\n",
       " 'List06.txt',\n",
       " 'List03.txt',\n",
       " 'word_freq.json',\n",
       " 'word_list.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = './raw_words'\n",
    "file_list = os.listdir(base_dir)\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "quantitative-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List0X.txt\n",
    "list_type_files = [filename for filename in file_list if 'List' in filename]\n",
    "for file in list_type_files:\n",
    "    with open(os.path.join(base_dir,file)) as f:\n",
    "        output = [x for x in f.read().splitlines() if x!='' and x.startswith('/')==False]\n",
    "    words = [line.strip().lower() for line in output if '.' not in line]\n",
    "    all_words.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dangerous-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_list_magoosh.txt\n",
    "with open(os.path.join(base_dir,'word_list_magoosh.txt')) as f:\n",
    "    output = f.read().splitlines()\n",
    "    magoosh_words = [line.strip().lower() for line in output]\n",
    "all_words.extend(magoosh_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "foster-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_list.txt\n",
    "with open(os.path.join(base_dir,'word_list.txt')) as f:\n",
    "#     output = [x.strip() for x in f.read().splitlines() if len(x)!=0 and '(' not in x and ')' not in x and '*' not in x and '-' not in x and ':' not in x and '/' not in x]\n",
    "    my_new_word_list = [x.strip().lower() for x in f.read().splitlines() if len(x.strip().split())==1 and '(' not in x and ')' not in x and  '-' not in x and '*' not in x and '.' not in x and not x.isupper()]\n",
    "all_words.extend(my_new_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "differential-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "#words_with_meanings.json\n",
    "with open(os.path.join(base_dir,'words_with_meanings.json')) as f:\n",
    "    output = json.load(f)\n",
    "for key in output:\n",
    "    all_words.extend([x['word'].lower() for x in output[key]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "forbidden-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_freq.json\n",
    "with open(os.path.join(base_dir,'word_freq.json')) as f:\n",
    "    output = json.load(f)\n",
    "fresher_word_list = [x for x in list(output.keys()) if len(x.split())==1]\n",
    "all_words.extend(fresher_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "extraordinary-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_list_essential.txt\n",
    "with open(os.path.join(base_dir,'word_list_essential.txt')) as f:\n",
    "    my_new_word_list_essential = [x.strip().lower() for x in f.read().splitlines() if len(x.strip().split())==1 and '(' not in x and ')' not in x and  '-' not in x and '*' not in x and '.' not in x and not x.isupper()]\n",
    "all_words.extend(my_new_word_list_essential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "therapeutic-italy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_list_pylike.txt\n",
    "with open(os.path.join(base_dir,'word_list_pylike.txt')) as f:\n",
    "    pylike_word_list = [x.strip().split('\\'')[1] for x in f.read().splitlines() if 'Word(' in x and len(x.strip().split('\\''))>1]\n",
    "all_words.extend([x.strip().lower() for x in pylike_word_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "laughing-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This has frequency data\n",
    "\n",
    "import requests\n",
    "request_text = requests.get('https://www.vocabulary.com/lists/128536').text\n",
    "try: \n",
    "    from BeautifulSoup import BeautifulSoup\n",
    "except ImportError:\n",
    "    from bs4 import BeautifulSoup\n",
    "html = request_text\n",
    "parsed_html = BeautifulSoup(html)\n",
    "html_list = list(parsed_html.body.find_all('li', attrs={'class':'entry learnable'}))\n",
    "\n",
    "large_af_word_list = [str(x).split('\\\"')[9] for x in html_list]\n",
    "all_words.extend(large_af_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "demographic-georgia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6024"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.dump(list(set([word.lower() for word in all_words])),open('really_all_words.pkl','wb'))\n",
    "all_words = pickle.load(open('really_all_words.pkl','rb'))\n",
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-sessions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented code\n",
    "#     word_dict = {}\n",
    "#     value = []\n",
    "\n",
    "#         word_dict[line] = tuple(value)\n",
    "#         value = []\n",
    "#         continue\n",
    "#     tokens = line.split('.')\n",
    "#     value.extend([x.strip() for x in tokens])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
